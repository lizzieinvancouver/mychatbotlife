\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{parskip}

\def\labelitemi{--}
\parindent=0pt

\begin{document}
\bibliographystyle{/Users/Lizzie/Documents/EndnoteRelated/Bibtex/styles/besjournals}
\renewcommand{\refname}{\CHead{}}

% \setlength{\parindent}{0cm}
% \setlength{\parskip}{5pt}

\emph{My life as a chatbot}% I've been mistaken for a chatbot

% https://www.science.org/content/page/science-information-authors
% https://www.nature.com/nature/for-authors/other-subs -- Careers seems good
% Thinking 800 words or less would be ideal -- currently 835 words

Like many people, I find writing a paper a somewhat painful process. % I generally have to will myself to do it and it rarely feels fluid or easy. 
Like some people, I comfort myself by reading books on how to write---both to be comforted by how much the authors of such books stress that writing is generally slow and difficult, and to find ways improve my writing. My current strategy involves various ways to will myself to write, and a general process of multiple outlines, then a first draft, then much revising. I try to force this approach on my students, even though I know it is not easy. I asked a graduate student once about this process, and he remarked that it was not easy, but the end result were reviews that almost always mentioned how well written the paper was, which seemed worth it. 
% then a first draft (usually a mix of plodding with punctuated bursts),
%  I try to force this approach on my students---finding little success in other approaches---

Imagine my surprise then when I received reviews back that declared a recently submitted paper of mine a chatGPT creation. One reviewer wrote that it was `obviously Chat GPT creation' and the handling editor vaguely agreed, saying that they found `the writing style unusual.' Surprise was one emotion, so was shock, dismay and a flood of confusion and alarm. Given how much work goes into writing a paper, it was quite a hit to be called a chatbot---especially in short order without any evidence. 

I hadn't written a word of the manuscript with chatGPT and I rapidly tried to think through how to prove my case. I could show my commits on GitHub (with commit messages including `finally writing!' and `Another 25 mins of writing progress!' that I never thought I would share), I could try to figure out to compare the writing style of my pre-chatGPT papers on this topic to the current submission, maybe I could ask chatGPT if it thought I it wrote the paper.... But then I realized I would be spending my scientific time trying to prove I am not a chatbot, which seemed a bad outcome to this whole situation. Eventually,  like all mature adults, I decided what I most wanted to do was pick up my ball (manuscript) and march off the playground in a small fury. How dare they? 

Before I did this, I decided to get some perspectives from others---colleagues who work on data fraud, co-authors on the paper and others, and I found most agreed with my alarm. One put it most succinctly to me: `All scientific criticism is admissible, but this is a different matter.' 

I realized these reviews captured both something inherently broken about the peer review process and---more importantly to me---about how AI could corrupt science without even trying. As a society, we're paranoid about AI taking over and we're trying to put in structures so it doesn't. But also so it helps where it should, and maybe that will be writing parts of papers. But here, chatGPT was not part of my process and yet it had prejudiced the whole process simply by its very presence in the world. 

So much of science is built on trust and faith in the scientific ethics and integrity of our colleagues. We mostly trust others did not fabricate their data, and I trust people do not (yet) write their papers or grants using large language models without telling me. I wouldn't accuse someone of data fraud or p-hacking without some evidence and logical thought, but a colleague felt it was easy enough to accuse me of writing fraud. Indeed, the reviewer wrote, `It is obviously Chat GPT creation, there is nothing wrong using help ....' So it seems, perhaps, that they did not see this a harsh accusation, and the editor thought nothing of passing it along and echoing it. They also felt confident that they could discern my writing from AI---but they couldn't. It seems we lack a common standard of what is fraud, and what accusations can be made flippantly.

I don't want a world in which we can't call out fraud and misconduct in science. Currently, the costs to the people who call out data fraud seem too high to me, and the consequences for it too low (people should lose tenure for egregious data fraud in my book). This is part of why my lab works hard at data transparency, data sharing and reproducibility---we're likely slower at publishing than other labs because of this---but it's a standard I think we should all expect. But I am worried about a world in which my colleagues easily declare my work AI-generated, and the editors and journal editor shuffle along the review and invite a resubmission if I so choose. It suggests not only a world in which the reviewers and editors have no faith in my scientific integrity but also where we may have radically different standards for scientific ethics. Such a world seems easy for chatGPT to destroy without even trying---unless we raise our standards. 

\end{document}


Ridiculous ... 

There was not much in these reviews about what to change, and there was nothing in them about what made the editor and reviewer so sure the paper was written by a chatbot other than

Why the accusation? Maybe they think fraud through writing is less of an accusation of fraud through fake data? 

Plodding with bursts, github commits

... ``For my part, I find it almost a defamatory accusation, as it undermines the intellectual honesty of the authors.\\
All scientific criticism is admissible, but this is a different matter.''

Distinguishing truth/reality from AI; people think they can, but they can't

My lab focuses on data transparency and data sharing.

Review process was flawed -- prejudiced, no due diligence

So much of science is built on trust and faith. We assume people did not fabricate data.

Scientific standards and ethics need to be strong to combat this; they're not naturally, and they never will be.

We're paranoid about AI taking over but it doesn't have to do anything than be out there to corrupt the system. 



Help: I've been mistaken for a chatbot

You never really know what direction papers will go, which is one thing I like about the writing process. Things I dislike include ....


Thierry quote:

Distinguish truth from reality: people thing they can, but they can't.

My lab is focused on data transparency and data sharing.

Review process was flawed (prejudiced, no due diligence)

We're paranoid about AI taking over, but it doesn't have to do anything other than be out there to corrupt the system... which is easy to corrupt.

So much of science if built on trust/ethics -- that needs to be strong to combat AI, but truth/ethics in science is not naturally that strong and probably never will be. 

I have github commits to try to show I wrote my paper, but what about others? (Include some git commits.)

