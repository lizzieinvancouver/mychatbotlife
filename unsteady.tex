\documentclass[11pt]{article}
\usepackage[top=1.00in, bottom=1.0in, left=1in, right=1in]{geometry}
\renewcommand{\baselinestretch}{1.1}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{parskip}

\def\labelitemi{--}
\parindent=0pt

\begin{document}
\bibliographystyle{/Users/Lizzie/Documents/EndnoteRelated/Bibtex/styles/besjournals}
\renewcommand{\refname}{\CHead{}}

% \setlength{\parindent}{0cm}
% \setlength{\parskip}{5pt}

\emph{I have been mistaken for a chatbot}% I've been mistaken for a chatbot

% https://www.science.org/content/page/science-information-authors
% https://www.nature.com/nature/for-authors/other-subs -- Careers seems good
% Thinking 800 words or less would be ideal -- currently 835 words

I have just been accused of scientific fraud, but not data fraud. I guess that I am relieved it's not data fraud because my lab works hard at data transparency, data sharing and reproducibility. We're likely slower at publishing than other labs because of this, but I also like to think it makes fabricating data in one way or another harder. 

What I have just been accused of is writing fraud. This hurts, because, like many people, I find writing a paper a somewhat painful process. % I generally have to will myself to do it and it rarely feels fluid or easy. 
Like some people, I comfort myself by reading books on how to write---both to be comforted by how much the authors of such books stress that writing is generally slow and difficult, and to find ways to improve my writing. My current strategy involves willing myself to write, and a general process of multiple outlines, then a first draft, then much revising. I try to force this approach on my students, even though I know it is not easy. I asked a graduate student once about this and he remarked that the end result were reviews that almost always mentioned how well written the paper was, which seemed worth it. I take some small pride in trying to write well.
% then a first draft (usually a mix of plodding with punctuated bursts),
%  I try to force this approach on my students---finding little success in other approaches---

Imagine my surprise then when I received reviews back that declared a recently submitted paper of mine a chatGPT creation. One reviewer wrote that it was `obviously Chat GPT' and the handling editor vaguely agreed, saying that they found `the writing style unusual.' Surprise was one emotion, so was shock, dismay and a flood of confusion and alarm. Given how much work goes into writing a paper, it was quite a hit to be accused of being a chatbot---especially in short order without any evidence, and given the blood sweat and tears that accompany the writing of almost all my manuscripts. 

I hadn't written a word of the manuscript with chatGPT and I rapidly tried to think through how to prove my case. I could show my commits on GitHub (with commit messages including `finally writing!' and `Another 25 mins of writing progress!' that I never thought I would share), I could try to figure out how to compare the writing style of my pre-chatGPT papers on this topic to the current submission, maybe I could ask chatGPT if it thought I it wrote the paper.... But then I realized I would be spending my time trying to prove I am not a chatbot, which seemed a bad outcome to this whole situation. Eventually,  like all mature adults, I decided what I most wanted to do was pick up my ball (manuscript) and march off the playground in a small fury. How dare they? 

Before I did this, I decided to get some perspectives from others---researchers who work on data fraud, co-authors on the paper and colleagues, and I found most agreed with my alarm. One put it most succinctly to me: `All scientific criticism is admissible, but this is a different matter.' 

I realized these reviews captured both something inherently broken about the peer review process and---more importantly to me---about how AI could corrupt science without even trying. We're paranoid about AI taking over us weak humans and we're trying to put in structures so it doesn't. But we're also trying to develop AI so it helps where it should, and maybe that will be writing parts of papers. Here, chatGPT was not part of my process and yet it had prejudiced the whole process simply by its existential presence in the world. I realized I was at once annoyed at being mistaken for a chatbot and horrified that reviewers and editors were not more outraged at the idea that someone had submitted AI generated text.

% ADD: an unresolved conflict in being annoyed at being mistaken for a chatbot and annoyed that reviewers/editors were not more outraged at the idea that someone had submitted AI generated text.
So much of science is built on trust and faith in the scientific ethics and integrity of our colleagues. We mostly trust others did not fabricate their data, and I trust people do not (yet) write their papers or grants using large language models without telling me. I wouldn't accuse someone of data fraud or $p$-hacking without some evidence, but a reviewer felt it was easy enough to accuse me of writing fraud. Indeed, the reviewer wrote, `It is obviously [a] Chat GPT creation, there is nothing wrong using help ....' So it seems, perhaps, that they did not see this as a harsh accusation, and the editor thought nothing of passing it along and echoing it, but they had effectively accused me of lying and fraud in deliberately presenting AI generated text as my own. They also felt confident that they could discern my writing from AI---but they couldn't. % It seems we lack a common standard of what is fraud, and what accusations can be made flippantly.

We need to be able to call out fraud and misconduct in science. Currently, the costs to the people who call out data fraud seem too high to me, and the consequences for being caught too low (people should lose tenure for egregious data fraud in my book). But I am worried about a world in which a reviewer can casually declare my work AI-generated, and the editors and journal editor simply shuffle along the review and invite a resubmission if I so choose. It suggests not only a world in which the reviewers and editors have no faith in the scientific integrity of submitting authors---me---but also an acceptance of a world where ethics are negotiable. Such a world seems easy for chatGPT to corrupt without even trying---unless we raise our standards. 

\end{document}


Ridiculous ... 

There was not much in these reviews about what to change, and there was nothing in them about what made the editor and reviewer so sure the paper was written by a chatbot other than

Why the accusation? Maybe they think fraud through writing is less of an accusation of fraud through fake data? 

Plodding with bursts, github commits

... ``For my part, I find it almost a defamatory accusation, as it undermines the intellectual honesty of the authors.\\
All scientific criticism is admissible, but this is a different matter.''

Distinguishing truth/reality from AI; people think they can, but they can't

My lab focuses on data transparency and data sharing.

Review process was flawed -- prejudiced, no due diligence

So much of science is built on trust and faith. We assume people did not fabricate data.

Scientific standards and ethics need to be strong to combat this; they're not naturally, and they never will be.

We're paranoid about AI taking over but it doesn't have to do anything than be out there to corrupt the system. 


% Notes below off the scrap of paper .... but I misplaced them so ending up typing them twice. 
Help: I've been mistaken for a chatbot

You never really know what direction papers will go, which is one thing I like about the writing process. Things I dislike include ....


Thierry quote:

Distinguish truth from reality: people thing they can, but they can't.

My lab is focused on data transparency and data sharing.

Review process was flawed (prejudiced, no due diligence)

We're paranoid about AI taking over, but it doesn't have to do anything other than be out there to corrupt the system... which is easy to corrupt.

So much of science if built on trust/ethics -- that needs to be strong to combat AI, but truth/ethics in science is not naturally that strong and probably never will be. 

I have github commits to try to show I wrote my paper, but what about others? (Include some git commits.)

